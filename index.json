[{"authors":["admin"],"categories":null,"content":"üçÄ Junjie Ye is currently a Master student in Mechanical Engineering at Tongji University, Shanghai, China, where he received his B.E. degree with Excellent Graduate Honor in 2020.\nSince Jan. 2020, I have been working on intelligent visual perception for UAV supervised by Prof. Changhong Fu. In 2021, I served as a research intern at the JD-AR Vision Learning Group, JD.COM Inc., advised by Shan An. In addition, I am collaborating closely with Dr. Danda Pani Paudel (Computer Vision Lab, ETHz) and Prof. Geng Lu (UAV Lab, Tsinghua University). My research interests include visual object tracking, deep learning, and robotics.\nFor more info, please refer to my latest CV.\n","date":1643702400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1643702400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jayye99.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"üçÄ Junjie Ye is currently a Master student in Mechanical Engineering at Tongji University, Shanghai, China, where he received his B.E. degree with Excellent Graduate Honor in 2020.\nSince Jan. 2020, I have been working on intelligent visual perception for UAV supervised by Prof. Changhong Fu. In 2021, I served as a research intern at the JD-AR Vision Learning Group, JD.COM Inc., advised by Shan An. In addition, I am collaborating closely with Dr.","tags":null,"title":"Junjie Ye","type":"authors"},{"authors":["Junjie Ye","Changhong Fu","Guangze Zheng","Danda Pani Paudel","Guang Chen"],"categories":null,"content":"\n Illustration of the proposed unsupervised domain adaptation framework for nighttime aerial tracking.\n\n","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"10d2a58c9f73c59e61d4a01ed1b791b3","permalink":"https://jayye99.github.io/publication/2022_cvpr_udat/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/2022_cvpr_udat/","section":"publication","summary":"Previous advances in object tracking mostly reported on favorable illumination circumstances while neglecting performance at nighttime, which significantly impeded the development of related aerial robot applications. This work instead develops a novel unsupervised domain adaptation framework for nighttime aerial tracking (named UDAT). Specifically, a unique object discovery approach is provided to generate training patches from raw nighttime tracking videos. To tackle the domain discrepancy, we employ a Transformer-based bridging layer post to the feature extractor to align image features from both domains. With a Transformer day/night feature discriminator, the daytime tracking model is adversarially trained to track at night. Moreover, we construct a pioneering benchmark namely NAT2021 for unsupervised domain adaptive nighttime tracking, which comprises a test set of 180 manually annotated tracking sequences and a train set of over 285k unlabelled nighttime tracking frames. Exhaustive experiments demonstrate the robustness and domain adaptability of the proposed framework in nighttime aerial tracking.","tags":["Unsupervised domain adaptation","Nighttime aerial tracking","Benchmark","Transformer"],"title":"Unsupervised Domain Adaptation for Nighttime Aerial Tracking","type":"publication"},{"authors":["Changhong Fu","Sihang Li","Xinnan Yuan","Junjie Ye","Ziang Cao","Fangqiang Ding"],"categories":null,"content":"\nOverview of our Ad2Attack pipeline.\n\n","date":1643702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643702400,"objectID":"4c886f02ab35d2f45fd991574daea621","permalink":"https://jayye99.github.io/publication/2022_icra_ad2attack/","publishdate":"2022-01-31T08:00:00Z","relpermalink":"/publication/2022_icra_ad2attack/","section":"publication","summary":"Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related applications, which leads to a highly demanding requirement on the robustness of UAV trackers. However, adding imperceptible perturbations can easily fool the tracker and cause tracking failures. This risk is often overlooked and rarely researched at present. Therefore, to help increase awareness of the potential risk and the robustness of UAV tracking, this work proposes a novel adaptive adversarial attack approach, i.e., Ad2Attack, against UAV object tracking. Specifically, adversarial examples are generated online during the resampling of the search patch image, which leads trackers to lose the target in the following frames. Ad2Attack is composed of a direct downsampling module and a super-resolution upsampling module with adaptive stages. A novel optimization function is proposed for balancing the imperceptibility and efficiency of the attack. Comprehensive experiments on several well-known benchmarks and real-world conditions show the effectiveness of our attack method, which dramatically reduces the performance of the most advanced Siamese trackers.","tags":["Visual tracking","Unmanned aerial vehicles","Adversarial attack"],"title":"Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Ziang Cao","Shan An","Guangze Zheng","Bowen Li"],"categories":null,"content":" Overall performance of SOTA trackers with the proposed SCT enabled (markers in a dark color) or not (markers in a light color) in the newly constructed nighttime UAV tracking benchmark\u0026mdash;DarkTrack2021. SCT significantly boosts the nighttime tracking performance of trackers in a plug-and-play manner. \n","date":1641168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168000,"objectID":"e9308ae6e15c9f7ab24895829b29945f","permalink":"https://jayye99.github.io/publication/2022_ral_sct/","publishdate":"2022-01-03T00:00:00Z","relpermalink":"/publication/2022_ral_sct/","section":"publication","summary":"Most previous progress in object tracking is realized in daytime scenes with favorable illumination. State-of-the-arts can hardly carry on their superiority at night so far, thereby considerably blocking the broadening of visual tracking-related unmanned aerial vehicle (UAV) applications. To realize reliable UAV tracking at night, a spatial-channel Transformer-based low-light enhancer (namely SCT), which is trained in a novel task-inspired manner, is proposed and plugged prior to tracking approaches. To achieve semantic-level low-light enhancement targeting the high-level task, the novel spatial-channel attention module is proposed to model global information while preserving local context. In the enhancement process, SCT denoises and illuminates nighttime images simultaneously through a robust non-linear curve projection. Moreover, to provide a comprehensive evaluation, we construct a challenging nighttime tracking benchmark, namely DarkTrack2021, which contains 110 challenging sequences with over 100K frames in total. Evaluations on both the public UAVDark135 benchmark and the newly constructed DarkTrack2021 benchmark show that the task-inspired design enables SCT with significant performance gains for nighttime UAV tracking compared with other top-ranked low-light enhancers. Real-world tests on a typical UAV platform further verify the practicability of the proposed approach.","tags":["Unmanned aerial vehicle","Nighttime tracking","Low-light enhancement","Transformer"],"title":"Tracker Meets Night: A Transformer Enhancer for UAV Tracking","type":"publication"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"a3ca2af2b6970d3faedb710a28839ba4","permalink":"https://jayye99.github.io/project/darktrack2021/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/darktrack2021/","section":"project","summary":"DarkTrack2021 is a nighttime tracking benchmark comprises 110 challenging sequences with 100K frames in total.","tags":["UAV dark tracking","Benchmark"],"title":"DarkTrack2021","type":"project"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"7c217d7a0d9af29d165dd79e76d663fe","permalink":"https://jayye99.github.io/project/uamt100/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/uamt100/","section":"project","summary":"UAMT100 benchmark is built for UAM tracking method evaluation. It contains 100 image sequences recorded on a flying UAM platform.","tags":["UAM tracking","Benchmark"],"title":"UAMT100","type":"project"},{"authors":["Shan An","Guangfu Che","Jinghao Guo","Haogang Zhu","Junjie Ye","Fangru Zhou","Zhaoqi Zhu","Dong Wei","Aishan Liu","Wei Zhang"],"categories":null,"content":" Virtual try-on results obtained by ARShoe.\n","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"1fe5325a2cdf2c9c0b7312d0f5c96041","permalink":"https://jayye99.github.io/publication/2021_acmmm_arshoe/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/2021_acmmm_arshoe/","section":"publication","summary":"Virtual try-on technology enables users to try various fashion items using augmented reality and provides a convenient online shopping experience. However, most previous works focus on the virtual try-on for clothes while neglecting that for shoes, which is also a promising task. To this concern, this work proposes a real-time augmented reality virtual shoe try-on system for smartphones, namely ARShoe. Specifically, ARShoe adopts a novel multi-branch network to realize pose estimation and segmentation simultaneously. A solution to generate realistic 3D shoe model occlusion during the try-on process is presented. To achieve a smooth and stable tryon effect, this work further develop a novel stabilization method. Moreover, for training and evaluation, we construct the very first large-scale foot benchmark with multiple virtual shoe try-on taskrelated labels annotated. Exhaustive experiments on our newly constructed benchmark demonstrate the satisfying performance of ARShoe. Practical tests on common smartphones validate the real-time performance and stabilization of the proposed approach","tags":["Augmented reality","Visual shoe try-on"],"title":"ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":" Fig. 1 Overview of the HiFT tracker. Fig. 2 Framework of the hierarchical feature transformer. \n","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"c9ab8c003b230e357d009edabed830f2","permalink":"https://jayye99.github.io/publication/2021_iccv_hift/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/2021_iccv_hift/","section":"publication","summary":"Siamese-based visual tracking methods generally execute the classification and regression of the target object based on the similarity maps. However, existing works either solely employ a single map generated by the last convolutional layer which degrades the localization accuracy, or separately use multiple maps for decision making, introducing intractable computations for aerial mobile platforms. In this work, we propose an efficient and effective hierarchical feature transformer (HiFT) in Siamese tracking. Hierarchical similarity maps generated by multi-level convolutional layers are fed into a feature transformer network. Not only the global contextual information can be raised, facilitating the target search, but also our end-to-end architecture with the transformer can learn the inter-dependencies among multi-level features, and discover a tracking-tailored feature space with strong discriminability due to the interactive fusion of spatial (early layers) and semantics cues (deep layers). Comprehensive evaluations on aerial benchmarks have proven the effectiveness of HiFT, and the real-world tests on the aerial platform have validated its practicability and robustness with a real-time speed.","tags":["Siamese network","Real-time object tracking","Unmanned aerial vehicles","Transformer"],"title":"HiFT: Hierarchical Feature Transformer for Aerial Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Guangze Zheng","Ziang Cao","Bowen Li"],"categories":null,"content":" Tracking performance comparison in a typical dark scene with the proposed DarkLighter module activated (in red) or not (in pink).\n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"c98ba9d164d6dc005d7f59a9f3dff2f1","permalink":"https://jayye99.github.io/publication/2021_iros_darklighter/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_darklighter/","section":"publication","summary":"Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, \\textit{i.e.}, ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability.","tags":["Low-light enhancement","Visual tracking","Unmanned aerial vehicles"],"title":"DarkLighter: Light Up the Darkness for UAV Tracking","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":" The overview of the SiamAPN++ tracker.\n","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"0c32adec2cf596405b8de13633f93db3","permalink":"https://jayye99.github.io/publication/2021_iros_siamapn++/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_siamapn++/","section":"publication","summary":"Recently, the Siamese-based method has stood out from multitudinous tracking methods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to various special challenges in UAV tracking, e.g., severe occlusion, and fast motion, most existing Siamese-based trackers hardly combine superior performance with high efficiency. To this concern, in this paper, a novel attentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking. By virtue of the attention mechanism, the attentional aggregation network (AAN) is conducted with self-AAN and cross-AAN, raising the expression ability of features eventually. The former AAN aggregates and models the self-semantic interdependencies of the single feature map via spatial and channel dimensions. The latter aims to aggregate the cross-interdependencies of different semantic features including the location information of anchors. In addition, the dual features version of the anchor proposal network is proposed to raise the robustness of proposing anchors, increasing the perception ability to objects with various scales. Experiments on two well-known authoritative benchmarks are conducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA trackers. Besides, real-world tests onboard a typical embedded platform demonstrate that SiamAPN++ achieves promising tracking results with real-time speed.","tags":["Siamese network","Visual tracking","Unmanned aerial vehicles"],"title":"SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Fuling Lin","Fangqiang Ding","Shan An","Geng Lu"],"categories":null,"content":" Overall flowchart of the proposed MRCF. \n","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"2dc6df5d1fe73a4908c6864fe515e2ba","permalink":"https://jayye99.github.io/publication/2021_tie_mrcf_tracker/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/publication/2021_tie_mrcf_tracker/","section":"publication","summary":"As a sort of model-free tracking approach, discriminative correlation filter (DCF)-based trackers have shown prominent performance in unmanned aerial vehicle (UAV) tracking. Nevertheless, typical DCFs acquire all samples oriented to filter training merely from the current frame by cyclic shift operation in the spatial domain but ignore the consistency between samples across the timeline. The lack of temporal cues restricts the performance of DCFs under object appearance variations arising from object/UAV motion, scale variations, and viewpoint changes. Besides, many existing methods commonly neglect the channel discrepancy in object position estimation and generally treat all channels equally, thus limiting the further promotion of the tracking discriminability. To these concerns, this work proposes a novel tracking approach based on a multi-regularized correlation filter, i.e., MRCF tracker. By regularizing the deviation of responses and the reliability of channels, the tracker enables smooth response variations and adaptive channel weight distributions simultaneously, leading to favorable adaption to object appearance variations and enhancement of discriminability. Exhaustive experiments on five authoritative UAV-specific benchmarks validate the competitiveness and efficiency of MRCF against top-ranked trackers. Furthermore, we apply our proposed tracker to monocular UAV self-localization under air-ground robot coordination. Evaluations indicate the practicability of the presented method in UAV localization applications.","tags":["Unmanned aerial vehicle (UAV)","Model-free object tracking","Multi-regularized correlation filter","Vision-based UAV self-localization"],"title":"Multi-Regularized Correlation Filter for UAV Tracking and Self-Localization","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":" The workflow of SiamAPN. It is composed of four subnetworks and two stages, i.e., feature extraction network, feature fusion network, anchor proposal network, and classification\u0026amp;regression network. Stage-1 includes feature extraction network and anchor proposal network (APN). Stage-2 contains feature fusion network and classification\u0026amp;regression network. \n","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"f33f3154720a3c4a64b5aa1335e2e34f","permalink":"https://jayye99.github.io/publication/2021_tgrs_siamapn_ext/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/publication/2021_tgrs_siamapn_ext/","section":"publication","summary":"Object tracking approaches based on siamese network have demonstrated their huge potential in remote sensing field recently. Nevertheless, due to the limited computing resource of aerial platforms and special challenges in aerial tracking, most existing siamese-based methods can hardly meet the real-time and state-of-the-art performance at the same time. Consequently, a novel siamese-based method is proposed in this work for onboard real-time aerial tracking, i.e., SiamAPN. The proposed method is a no-prior two-stage method, i.e., stage-1 for proposing adaptive anchors to enhance the ability of object perception, stage-2 for fine-tuning the proposed anchors to obtain accurate results. Distinct from pre-defined fixed-sized anchors, our adaptive anchors are adapt automatically to accommodate the tracking object. Besides, the internal information of adaptive anchors is utilized to feedback SiamAPN for enhancing the object perception. Attributing to the feature fusion network, different semantic information is integrated, enriching the information flow. In the end, the regression and multi-classification operation refine the proposed anchors meticulously. Comprehensive evaluations on three well-known benchmarks have proven the superior performance of our approach. Moreover, to verify the practicability of the proposed method, SiamAPN is implemented in an onboard system. Real-world flight tests are conducted on aerial tracking specific scenarios, e.g., low resolution, fast motion, and long-term tracking, the results demonstrate the efficiency and accuracy of our approach, with a processing speed of over 30 frame/s. In addition, the image sequences in the real-world flight tests are collected and annotated as a new benchmark, i.e., UAVTrack112.","tags":["Real-time aerial tracking","Efficient siamese structure","Anchor proposal network","No-prior adaptive anchors","Onboard embedded processing","Real-world flight tests"],"title":"Onboard Real-Time Aerial Tracking with Efficient Siamese Anchor Proposal Network","type":"publication"},{"authors":["Bowen Li","Yiming Li","Junjie Ye","Changhong Fu","Hang Zhao"],"categories":null,"content":"  \n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"8f69f97ecf740fb99d1a9fb280cd8166","permalink":"https://jayye99.github.io/publication/2021_arxiv_pvt/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/2021_arxiv_pvt/","section":"publication","summary":"As a crucial robotic perception capability, visual tracking has been intensively studied recently. In the real-world scenarios, the onboard processing time of the image streams inevitably leads to a discrepancy between the tracking results and the real-world states. However, existing visual tracking benchmarks commonly run the trackers offline and ignore such latency in the evaluation. In this work, we aim to deal with a more realistic problem of latency-aware tracking. The state-ofthe-art trackers are evaluated in the aerial scenarios with new metrics jointly assessing the tracking accuracy and efficiency. Moreover, a new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation. Our latency-aware benchmark can provide a more realistic evaluation of the trackers for the robotic applications. Besides, exhaustive experiments have proven the effectiveness of the proposed predictive visual tracking baseline approach. Our code is on https://github.com/vision4robotics/LAE-PVT-master.","tags":["Unmanned aerial vehicle","Visual object tracking","Latency-aware tracking"],"title":"Predictive Visual Tracking: A New Benchmark and Baseline Approach","type":"publication"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":" Overall framework of the proposed ADTrack. ADTrack includes 3 stages: pretreatment, training, and detection, which are marked out by boxes in different colors. Dual filters, i.e., context filter and target-focused filter, training and detection follow routes in different colors. It can be seen that the final response shaded noises in context response, which indicates the validity of proposed dual filter.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"1433737504ce4cad2e1d5e6586c8e05f","permalink":"https://jayye99.github.io/publication/2021_icra_adtrack/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_adtrack/","section":"publication","summary":"Prior correlation filter (CF)-based tracking methods for unmanned aerial vehicles (UAVs) have virtually focused on tracking in the daytime. However, when the night falls, the trackers will encounter more harsh scenes, which can easily lead to tracking failure. In this regard, this work proposes a novel tracker with anti-dark function (ADTrack). The proposed method integrates an effcient and effective low-light image enhancer into a CF-based tracker. Besides, a target-aware mask is simultaneously generated by virtue of image illumination variation. The target-aware mask can be applied to jointly train a target-focused filter that assists the context filter for robust tracking. Specifically, ADTrack adopts dual regression, where the context filter and the target-focused filter restrict each other for dual filter learning. Exhaustive experiments are conducted on typical dark sceneries benchmark, consisting of 37 typical night sequences from authoritative benchmarks, i.e., UAVDark, and our newly constructed benchmark UAVDark70. The results have shown that ADTrack favorably outperforms other state-of-the-art trackers and achieves a real-time speed of 34 frames/s on a single CPU, thus greatly extending robust UAV tracking to night scenes.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Fuling Lin","Fangqiang Ding"],"categories":null,"content":" Tracking procedure of the proposed MSCF tracker. Dashed boxes denote the variables to be solved in the main regression. As MTF in the red box is generated from search region in frame k, it is applied to adjust the altitude value of the cruciform pedestal.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"2b51ed03d91fc1ed1d4cc28c2abf7736","permalink":"https://jayye99.github.io/publication/2021_icra_mscf_tracker/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_mscf_tracker/","section":"publication","summary":"Unmanned aerial vehicle (UAV) based visual tracking has been confronted with numerous challenges, e.g., object motion and occlusion. These challenges generally bring about target appearance mutations and cause tracking failure. However, most prevalent discriminative correlation filter (DCF) based trackers are insensitive to target mutations due to a predefined label, which concentrates on merely the centre of the target. Meanwhile, appearance mutations incited by occlusion or similar objects commonly lead to inevitable learning of erroneous information. To cope with appearance mutations, this paper proposes a novel DCF-based method to enhance the sensitivity and resistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal label is optimized jointly with the correlation filter and remains consistent with the previous label. Meanwhile, a novel measurement of mutations called mutation threat factor (MTF) is applied to correct the label dynamically. Through the revision of label into hybrid shape, MSCF can demonstrate preferable adaptability during appearance mutations. Considerable experiments are conducted on widely used UAV benchmarks. Results manifest the performance of MSCF tracker surpassing other 26 state-ofthe-art DCF-based and deep-based trackers. With a real-time speed of ~ 38 frames/s, the proposed approach is sufficient for UAV tracking commissions.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":"\nThe overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classification¬Æression network.\n\n","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"9e61feeea128ae22e4769a11f6af92fb","permalink":"https://jayye99.github.io/publication/2021_icra_siamapn/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_siamapn/","section":"publication","summary":"In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency, thereby impeding their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV). In this work, a novel two-stage siamese network-based method is proposed for aerial tracking, \\textit{i.e.}, stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) make tracker robust and general to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the state-of-the-art performance of our approach, with a speed of ~ 200 frames/s.","tags":["Visual tracking","Unmanned aerial vehicles","Anchor proposal network"],"title":"Siamese Anchor Proposal Network for High-Speed Aerial Tracking","type":"publication"},{"authors":null,"categories":null,"content":"","date":1614384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614384000,"objectID":"a8f5650c03497fa0a8aab6c57df848b4","permalink":"https://jayye99.github.io/project/pvt/","publishdate":"2021-02-27T00:00:00Z","relpermalink":"/project/pvt/","section":"project","summary":"Predictive visual tracking (PVT) is a latency-aware benchmark jointly assessing the tracking accuracy and efficiency of trackers.","tags":["UAV tracking","Latency-aware","Benchmark"],"title":"PVT","type":"project"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":" Pipeline of ADTrack. \n","date":1611446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611446400,"objectID":"a1378502c275eba58375066b33f246bc","permalink":"https://jayye99.github.io/publication/2021_arxiv_adtrack/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/publication/2021_arxiv_adtrack/","section":"publication","summary":"Visual object tracking, which is representing a major interest in image processing field, has facilitated numerous real world applications. Among them, equipping unmanned aerial vehicle (UAV) with real time robust visual trackers for all day aerial maneuver, is currently attracting incremental attention and has remarkably broadened the scope of applications of object tracking. However, prior tracking methods have merely focused on robust tracking in the well-illuminated scenes, while ignoring trackers' capabilities to be deployed in the dark. In darkness, the conditions can be more complex and harsh, easily posing inferior robust tracking or even tracking failure. To this end, this work proposed a novel discriminative correlation filter based tracker with illumination adaptive and anti dark capability, namely ADTrack. ADTrack firstly exploits image illuminance information to enable adaptability of the model to the given light condition. Then, by virtue of an efficient and effective image enhancer, ADTrack carries out image pretreatment, where a target aware mask is generated. Benefiting from the mask, ADTrack aims to solve a dual regression problem where dual filters, i.e., the context filter and target focused filter, are trained with mutual constraint. Thus ADTrack is able to maintain continuously favorable performance in all-day conditions. Besides, this work also constructed one UAV nighttime tracking benchmark UAVDark135, comprising of more than 125k manually annotated frames, which is also very first UAV nighttime tracking benchmark. Exhaustive experiments are extended on authoritative daytime benchmarks, i.e., UAV123 10fps, DTB70, and the newly built dark benchmark UAVDark135, which have validated the superiority of ADTrack in both bright and dark conditions on a single CPU.","tags":["Unmanned aerial vehicle","Visual object tracking","Discriminative correlation filter","Dark tracking benchmark","Image illumination based mask","Dual regression model"],"title":"All-Day Object Tracking for Unmanned Aerial Vehicle","type":"publication"},{"authors":null,"categories":null,"content":" Overview Exploit low-light enhancement, denoising, domain adaption technologies to adapt SOTA tracking approaches to low-light conditions. With slight computational consumption, enable UAV tracking at night.\nPapers with code Related works are presented as follows:\n Designed a Retinex-inspired plug-and-play deep low-light enhancer, dubbed DarkLighter, to light up the darkness for UAV tracking. Experiments on a dark tracking benchmark verify its effectiveness in several trackers and superiority against other SOTA general low-light enhancement algorithms, with sufficient real-time speed on an embedded system.   DarkLighter: Light up the Darkness for UAV Tracking\naccepted by IROS 2021\n  Constructed a spatial-channel Transformer (SCT) enhancer to facilitate nighttime UAV tracking in a task-inspired manner. Evaluations on the public UAVDark135 and the newly constructed DarkTrack2021 benchmarks demonstrate that the performance gains of SCT brought to nighttime UAV tracking surpass general low-light enhancers.   Tracker Meets Night: A Transformer Enhancer for UAV Tracking\naccepted by RA-L 2022\n Benchmarks We construct some pioneer benchmarks to serve for the development of nighttime object tracking: DarkTrack2021\u0026mdash;a nighttime tracking benchmark comprises 110 challenging sequences with 100K frames in total.\nUAVDark135\u0026mdash;a pioneering UAV dark tracking benchmark consists of 135 videos with a variety of objects.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"75df379dbb18a7c99436896c0df12f57","permalink":"https://jayye99.github.io/project/nighttime_tracking/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/nighttime_tracking/","section":"project","summary":"Adapt SOTA tracking approaches to low-light conditions.","tags":["Low-light","Aerial tracking"],"title":"Nighttime Aerial Tracking","type":"project"},{"authors":null,"categories":null,"content":" UAVDark135 is the very first UAV dark tracking benchmark dedicated to providing a comprehensive evaluation of tracking performance at night.\nUAVDark135 consists of 135 sequences, most of which were shot by a standard UAV at night, including more than 125k manually annotated frames. The benchmark covers a wide range of scenes, e.g., road, ocean, street, highway, and lakeside, including a large number of objects, such as person, car, building, athlete, truck, and bike.\nThe benchmark is available here (password: axci).\nUAVDark135 Tracking Benchmark A. Platform and Statistics Standing as the first UAV dark tracking benchmark, the UAVDark135 contains totally 135 sequences captured by a standard UAV2 at night. The benchmark includes various tracking scenes, e.g., crossings, t-junctions, road, highway, and consists of different kinds of tracked objects like people, boat, bus, car, truck, athletes, house, etc. To extent the covered scenes, the benchmark also contains some sequences from YouTube, which were shot on the sea. The total frames, mean frames, maximum frames, and minimum frames of the benchmark are 125466, 929, 4571, and 216 respectively, making it suitable for large-scale evaluation. The videos are captured at a frame-rate of 30 frames/s (FPS), with the resolution of 1920√ó1080.\nB. Annotation The frames in UAVDark135 are all manually annotated, where a sequence is completely processed by the same annotator to ensure consistency. Since in some dark scenes the object is nearly invisible, annotation process is much more strenuous. After the first round, 5 professional annotators carefully checked the results and made revision for several rounds to reduce errors as much as possible in nearly 2 months.\nSince the boundary contour of the object is not obvious in the dark, the result boxes of the first annotation fluctuates in continuous image frames. However, the actual motion process of the object should be smooth. In these considerations, we record the original annotation every 5 frames for the sequence with extremely severe vibration, and the results of the remaining frames are obtained by linear interpolation, which is closer to the position and scale variation of the real object.\nC. Attributes For more details, please refer to our paper.\n","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604102400,"objectID":"2d77a9e9fdf8af223bcf640d5b739add","permalink":"https://jayye99.github.io/project/uavdark135/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/project/uavdark135/","section":"project","summary":"A pioneering UAV dark tracking benchmark consists of 135 videos with a variety of objects.","tags":["UAV dark tracking","Benchmark"],"title":"UAVDark135","type":"project"},{"authors":["Changhong Fu","Junjie Ye","Juntao Xu","Yujie He","Fuling Lin"],"categories":null,"content":" Tracking procedure of the proposed IBRI tracker in the k-th frame. Historical interval responses are incorporated into the filter training phase after denoising by a novel disruptor-aware scheme based on response bucketing. \n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"06a2c27d3e614c9491702be79f0c1bcf","permalink":"https://jayye99.github.io/publication/2020_tgrs_ibri-tracker/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/publication/2020_tgrs_ibri-tracker/","section":"publication","summary":"Aerial object tracking approaches based on discriminative correlation filter (DCF) have attracted wide attention in the tracking community due to their impressive progress recently. Many studies introduce temporal regularization into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. However, existing temporal regularization approaches usually utilize the information of two consecutive frames, which are not robust enough due to limited information. Although some methods attempt to incorporate abundant training samples and generally improve the tracking performance, these improvements are at the expense of significantly increased computing consumption. Besides, most existing methods introduce historical information directly without denoising, which means background noises are also introduced into the filter training and may degrade the tracking accuracy. To tackle the drawbacks mentioned above, this work proposes a novel aerial object tracking approach to exploit disruptor-aware interval-based response inconsistency, i.e., IBRI tracker. The proposed method is able to incorporate historical interval information by utilizing responses in the filter training process, thereby obtaining a robust tracking performance while maintaining the real-time speed. Moreover, to reduce the disruptions caused by similar object, partial occlusion, and other challenging scenes, a novel disruptor-aware scheme based on response bucketing is introduced to detect the disruptor and enforce a spatial penalty for the disruptive area around the tracked object. Exhausted experiments on multiple well-known challenging aerial tracking benchmarks demonstrate the accuracy and robustness of the proposed IBRI tracker against other 35 state-of-the-art trackers. With a real-time speed of ~32 frames per second on a single CPU, the proposed approach can be applied for typical aerial platforms to achieve aerial visual object tracking efficiently.","tags":["Aerial object tracking","Discriminative correlation filter (DCF)","Temporal regularization","Historical frame information","Interval-based response inconsistency","Disruptor-aware bucketing"],"title":"Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking","type":"publication"},{"authors":null,"categories":null,"content":" Overview Construct robust Siamese network-based trackers for high-performance UAV tracking. Through enhancing the anchor proposal process, feature fusion strategy, attention mechanism, etc, we have developed several robust deep learning-based trackers for UAV. Papers with code Related works are presented as follows:\n Proposed the anchor proposal network (APN) for adaptive anchor proposing. Alleviated the hyper-parameters in anchor-based approaches and redundent anchors in anchor-free approaches simultaneously.   Siamese Anchor Proposal Network for High-Speed Aerial Tracking in ICRA 2021\nOnboard Real-Time Aerial Tracking with Efficient Siamese Anchor Proposal Network in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 2021\n  Integrated self-attention and cross-attention into SiamAPN, enhanced the perception ability for various scale objects of the proposed SiamAPN++. Evaluation on UAV tracking datasets and real-world onboard test demonstrate its effectiveness.   SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking in IROS 2021\n Benchmarks UAMT100\u0026mdash;a benchmark built for UAM tracking method evaluation which contains 100 image sequences recorded on a flying UAM platform.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"00301ba4e98f37690db7fa9d73ed2a9b","permalink":"https://jayye99.github.io/project/siamesetracking/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/project/siamesetracking/","section":"project","summary":"Construct robust Siamese network-based trackers for high-performance UAV tracking.","tags":["Siamese Network","Aerial tracking"],"title":"Siamese Network-Based UAV Tracking","type":"project"},{"authors":null,"categories":null,"content":" Overview Develop efficient and robust correlation filter (CF)-based trackers on CPU for UAV tracking in challenging scenarios. By mining temporal, spatial, and channel information properly, we have constructed several competitive tracking approaches while maintaining real-time performance on a single CPU.\nPapers with code Related works are presented as follows:\n Introduced the temporal regularization based on historical interval response inconsistency and the disruptor-aware mechanism based on response bucketing into the CF framework, realizing competitive performance on several UAV tracking-specific benchmarks.   Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n  Proposed the response deviation-aware regularization and the channel reliability aware regularization to make full use of the crucial information in response maps and reliable channels. Experiments demonstrate the proposed MRCF\u0026rsquo;s superiority in numerous challenging UAV tracking scenarios. Further, an original UAV self-localization system based on the proposed tracking approach is constructed.   Multi-Regularized Correlation Filter for UAV Tracking and Self-Localization submitted to IEEE Transactions on Industrial Electronics\n  Constructed a novel DCF-based tracker to enhance the sensitivity and resistance to mutations with an adaptive hybrid label. Considerable experiments on widely used UAV tracking benchmarks demonstrate its effectiveness.   Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label in ICRA 2021\n ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"8a452e33ff898db0e058ada811a8f40d","permalink":"https://jayye99.github.io/project/cftracking/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/project/cftracking/","section":"project","summary":"Develop efficient and robust trackers on CPU for UAV tracking in challenging scenarios.","tags":["Correlation filter","Aerial tracking"],"title":"Correlation Filter-Based UAV Tracking","type":"project"}]